{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import keras_cv\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from keras.utils import load_img, img_to_array\n",
    "import xml.etree.ElementTree as elemTree\n",
    "from luketils import visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_PATH = 'checkpoint2/'\n",
    "INFERENCE_CHECKPOINT_PATH = CHECKPOINT_PATH\n",
    "\n",
    "class_ids = [\n",
    "    \"ok\",\n",
    "    \"nok\"\n",
    "]\n",
    "class_mapping = dict(zip(range(len(class_ids)), class_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_image(images, boxes):\n",
    "    visualization.plot_bounding_box_gallery(\n",
    "        images,\n",
    "        value_range=(0, 255),\n",
    "        bounding_box_format='xywh',\n",
    "        y_true=boxes,\n",
    "        scale=4,\n",
    "        rows=8,\n",
    "        cols=5,\n",
    "        show=True,\n",
    "        thickness=4,\n",
    "        font_scale=1,\n",
    "        class_mapping=class_mapping,\n",
    "    )\n",
    "\n",
    "def img_load(fname):\n",
    "    img = img_to_array(load_img('images/trains/' + fname + '.jpg'))\n",
    "    return img\n",
    "\n",
    "def box_load(fname):\n",
    "    tree = elemTree.parse('images/trains/' + fname + '.xml')\n",
    "    object = tree.find('object')\n",
    "    name = object.find('name').text\n",
    "    bndbox = object.find('bndbox')\n",
    "    xmin = bndbox.find('xmin').text\n",
    "    ymin = bndbox.find('ymin').text\n",
    "    xmax = bndbox.find('xmax').text\n",
    "    ymax = bndbox.find('ymax').text\n",
    "    x = int(xmin)\n",
    "    y = int(ymin)\n",
    "    w = int(xmax) - int(xmin)\n",
    "    h = int(ymax) - int(ymin)\n",
    "    k = 0\n",
    "    for key, value in class_mapping.items():\n",
    "        if value == name:\n",
    "            k = key\n",
    "    return np.expand_dims(np.array([x, y, w, h, k]), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "bounding_boxes = []\n",
    "\n",
    "datasets = []\n",
    "\n",
    "fnames = ['ok', 'nok']    \n",
    "for fname in fnames:\n",
    "    image = img_load(fname)\n",
    "    box = box_load(fname)\n",
    "    datasets.append({ 'image': image, 'box': box })\n",
    "    for i in range(20):\n",
    "        rand_augment = keras_cv.layers.RandAugment(\n",
    "            value_range=(0, 255), augmentations_per_image=3, magnitude=0.5\n",
    "            , geometric=False\n",
    "        )\n",
    "        datasets.append({ 'image': rand_augment(image).numpy(), 'box': box })\n",
    "\n",
    "for data in datasets:\n",
    "    images.append(data['image'])\n",
    "    bounding_boxes.append(data['box'])\n",
    "\n",
    "images = np.array(images)\n",
    "bounding_boxes = np.array(bounding_boxes)\n",
    "# images = tf.convert_to_tensor(\n",
    "#     images, dtype=tf.float32\n",
    "# )\n",
    "bounding_boxes = tf.ragged.constant(bounding_boxes, dtype=tf.float32)\n",
    "\n",
    "# visualize_image(images, boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {'images': images, 'bounding_boxes': bounding_boxes}\n",
    "dataset = tf.data.Dataset.from_tensor_slices(inputs)\n",
    "dataset = dataset.batch(images.shape[0])\n",
    "example = next(iter(dataset))\n",
    "images, boxes = example['images'], example['bounding_boxes']\n",
    "image = images[0]\n",
    "boxes = boxes[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unbatching a tensor is only supported for rank >= 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [69], line 38\u001b[0m\n\u001b[0;32m     12\u001b[0m model\u001b[39m.\u001b[39mcompile(\n\u001b[0;32m     13\u001b[0m     classification_loss\u001b[39m=\u001b[39mkeras_cv\u001b[39m.\u001b[39mlosses\u001b[39m.\u001b[39mFocalLoss(from_logits\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, reduction\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mnone\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m     14\u001b[0m     box_loss\u001b[39m=\u001b[39mkeras_cv\u001b[39m.\u001b[39mlosses\u001b[39m.\u001b[39mSmoothL1Loss(l1_cutoff\u001b[39m=\u001b[39m\u001b[39m1.0\u001b[39m, reduction\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mnone\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     28\u001b[0m ],\n\u001b[0;32m     29\u001b[0m )\n\u001b[0;32m     31\u001b[0m callbacks \u001b[39m=\u001b[39m [\n\u001b[0;32m     32\u001b[0m     \u001b[39m# keras.callbacks.TensorBoard(log_dir=\"logs\"),\u001b[39;00m\n\u001b[0;32m     33\u001b[0m     keras\u001b[39m.\u001b[39mcallbacks\u001b[39m.\u001b[39mReduceLROnPlateau(patience\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m),\n\u001b[0;32m     34\u001b[0m     \u001b[39m# Uncomment to train your own RetinaNet\u001b[39;00m\n\u001b[0;32m     35\u001b[0m     keras\u001b[39m.\u001b[39mcallbacks\u001b[39m.\u001b[39mModelCheckpoint(CHECKPOINT_PATH, save_weights_only\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m),\n\u001b[0;32m     36\u001b[0m ]\n\u001b[1;32m---> 38\u001b[0m model\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m     39\u001b[0m     dataset,\n\u001b[0;32m     40\u001b[0m     validation_data\u001b[39m=\u001b[39;49m\u001b[39m0.3\u001b[39;49m,\n\u001b[0;32m     41\u001b[0m     epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m,\n\u001b[0;32m     42\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[0;32m     43\u001b[0m )\n\u001b[0;32m     44\u001b[0m model\u001b[39m.\u001b[39msave_weights(CHECKPOINT_PATH)\n",
      "File \u001b[1;32mc:\\Users\\seojuneng\\.conda\\envs\\vision\\lib\\site-packages\\keras_cv\\models\\object_detection\\object_detection_base_model.py:59\u001b[0m, in \u001b[0;36mObjectDetectionBaseModel.fit\u001b[1;34m(self, x, y, validation_data, validation_split, sample_weight, batch_size, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[39mif\u001b[39;00m validation_data \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     56\u001b[0m     val_x, val_y, val_sample \u001b[39m=\u001b[39m keras\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39munpack_x_y_sample_weight(\n\u001b[0;32m     57\u001b[0m         validation_data\n\u001b[0;32m     58\u001b[0m     )\n\u001b[1;32m---> 59\u001b[0m     validation_data \u001b[39m=\u001b[39m convert_inputs_to_tf_dataset(\n\u001b[0;32m     60\u001b[0m         x\u001b[39m=\u001b[39;49mval_x, y\u001b[39m=\u001b[39;49mval_y, sample_weight\u001b[39m=\u001b[39;49mval_sample, batch_size\u001b[39m=\u001b[39;49mbatch_size\n\u001b[0;32m     61\u001b[0m     )\n\u001b[0;32m     63\u001b[0m dataset \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39mmap(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencode_data, num_parallel_calls\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mAUTOTUNE)\n\u001b[0;32m     64\u001b[0m dataset \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39mprefetch(tf\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mAUTOTUNE)\n",
      "File \u001b[1;32mc:\\Users\\seojuneng\\.conda\\envs\\vision\\lib\\site-packages\\keras_cv\\models\\object_detection\\__internal__.py:54\u001b[0m, in \u001b[0;36mconvert_inputs_to_tf_dataset\u001b[1;34m(x, y, sample_weight, batch_size)\u001b[0m\n\u001b[0;32m     51\u001b[0m     inputs \u001b[39m=\u001b[39m (x, y)\n\u001b[0;32m     53\u001b[0m \u001b[39m# Construct tf.data.Dataset\u001b[39;00m\n\u001b[1;32m---> 54\u001b[0m dataset \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mDataset\u001b[39m.\u001b[39;49mfrom_tensor_slices(inputs)\n\u001b[0;32m     55\u001b[0m \u001b[39mif\u001b[39;00m batch_size \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mfull\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m     56\u001b[0m     dataset \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39mbatch(x\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\seojuneng\\.conda\\envs\\vision\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:814\u001b[0m, in \u001b[0;36mDatasetV2.from_tensor_slices\u001b[1;34m(tensors, name)\u001b[0m\n\u001b[0;32m    736\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[0;32m    737\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfrom_tensor_slices\u001b[39m(tensors, name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    738\u001b[0m   \u001b[39m\"\"\"Creates a `Dataset` whose elements are slices of the given tensors.\u001b[39;00m\n\u001b[0;32m    739\u001b[0m \n\u001b[0;32m    740\u001b[0m \u001b[39m  The given tensors are sliced along their first dimension. This operation\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    812\u001b[0m \u001b[39m    Dataset: A `Dataset`.\u001b[39;00m\n\u001b[0;32m    813\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 814\u001b[0m   \u001b[39mreturn\u001b[39;00m TensorSliceDataset(tensors, name\u001b[39m=\u001b[39;49mname)\n",
      "File \u001b[1;32mc:\\Users\\seojuneng\\.conda\\envs\\vision\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:4713\u001b[0m, in \u001b[0;36mTensorSliceDataset.__init__\u001b[1;34m(self, element, is_files, name)\u001b[0m\n\u001b[0;32m   4711\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tensors:\n\u001b[0;32m   4712\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mInvalid `element`. `element` should not be empty.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 4713\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_structure \u001b[39m=\u001b[39m nest\u001b[39m.\u001b[39;49mmap_structure(\n\u001b[0;32m   4714\u001b[0m     \u001b[39mlambda\u001b[39;49;00m component_spec: component_spec\u001b[39m.\u001b[39;49m_unbatch(), batched_spec)  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   4715\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_name \u001b[39m=\u001b[39m name\n\u001b[0;32m   4717\u001b[0m batch_dim \u001b[39m=\u001b[39m tensor_shape\u001b[39m.\u001b[39mDimension(\n\u001b[0;32m   4718\u001b[0m     tensor_shape\u001b[39m.\u001b[39mdimension_value(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tensors[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mget_shape()[\u001b[39m0\u001b[39m]))\n",
      "File \u001b[1;32mc:\\Users\\seojuneng\\.conda\\envs\\vision\\lib\\site-packages\\tensorflow\\python\\data\\util\\nest.py:228\u001b[0m, in \u001b[0;36mmap_structure\u001b[1;34m(func, *structure, **check_types_dict)\u001b[0m\n\u001b[0;32m    224\u001b[0m flat_structure \u001b[39m=\u001b[39m (flatten(s) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m structure)\n\u001b[0;32m    225\u001b[0m entries \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mflat_structure)\n\u001b[0;32m    227\u001b[0m \u001b[39mreturn\u001b[39;00m pack_sequence_as(\n\u001b[1;32m--> 228\u001b[0m     structure[\u001b[39m0\u001b[39m], [func(\u001b[39m*\u001b[39mx) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m entries])\n",
      "File \u001b[1;32mc:\\Users\\seojuneng\\.conda\\envs\\vision\\lib\\site-packages\\tensorflow\\python\\data\\util\\nest.py:228\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    224\u001b[0m flat_structure \u001b[39m=\u001b[39m (flatten(s) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m structure)\n\u001b[0;32m    225\u001b[0m entries \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mflat_structure)\n\u001b[0;32m    227\u001b[0m \u001b[39mreturn\u001b[39;00m pack_sequence_as(\n\u001b[1;32m--> 228\u001b[0m     structure[\u001b[39m0\u001b[39m], [func(\u001b[39m*\u001b[39;49mx) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m entries])\n",
      "File \u001b[1;32mc:\\Users\\seojuneng\\.conda\\envs\\vision\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:4714\u001b[0m, in \u001b[0;36mTensorSliceDataset.__init__.<locals>.<lambda>\u001b[1;34m(component_spec)\u001b[0m\n\u001b[0;32m   4711\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tensors:\n\u001b[0;32m   4712\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mInvalid `element`. `element` should not be empty.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   4713\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_structure \u001b[39m=\u001b[39m nest\u001b[39m.\u001b[39mmap_structure(\n\u001b[1;32m-> 4714\u001b[0m     \u001b[39mlambda\u001b[39;00m component_spec: component_spec\u001b[39m.\u001b[39;49m_unbatch(), batched_spec)  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   4715\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_name \u001b[39m=\u001b[39m name\n\u001b[0;32m   4717\u001b[0m batch_dim \u001b[39m=\u001b[39m tensor_shape\u001b[39m.\u001b[39mDimension(\n\u001b[0;32m   4718\u001b[0m     tensor_shape\u001b[39m.\u001b[39mdimension_value(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tensors[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mget_shape()[\u001b[39m0\u001b[39m]))\n",
      "File \u001b[1;32mc:\\Users\\seojuneng\\.conda\\envs\\vision\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_spec.py:229\u001b[0m, in \u001b[0;36mTensorSpec._unbatch\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    227\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_unbatch\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    228\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_shape\u001b[39m.\u001b[39mndims \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m--> 229\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mUnbatching a tensor is only supported for rank >= 1\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    230\u001b[0m   \u001b[39mreturn\u001b[39;00m TensorSpec(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_shape[\u001b[39m1\u001b[39m:], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dtype)\n",
      "\u001b[1;31mValueError\u001b[0m: Unbatching a tensor is only supported for rank >= 1"
     ]
    }
   ],
   "source": [
    "model = keras_cv.models.RetinaNet(\n",
    "    classes=20,\n",
    "    bounding_box_format='xywh',\n",
    "    backbone='resnet50',\n",
    "    backbone_weights='imagenet',\n",
    "    include_rescaling=True,\n",
    "    evaluate_train_time_metrics=False\n",
    ")\n",
    "\n",
    "model.backbone.trainable = False\n",
    "optimizer = tf.optimizers.SGD(global_clipnorm=10.0)\n",
    "model.compile(\n",
    "    classification_loss=keras_cv.losses.FocalLoss(from_logits=True, reduction=\"none\"),\n",
    "    box_loss=keras_cv.losses.SmoothL1Loss(l1_cutoff=1.0, reduction=\"none\"),\n",
    "    optimizer=optimizer,\n",
    "    metrics = [\n",
    "        keras_cv.metrics.COCOMeanAveragePrecision(\n",
    "        class_ids=range(2),\n",
    "        bounding_box_format=\"xywh\",\n",
    "        name=\"Mean Average Precision\",\n",
    "    ),\n",
    "    keras_cv.metrics.COCORecall(\n",
    "        class_ids=range(2),\n",
    "        bounding_box_format=\"xywh\",\n",
    "        max_detections=100,\n",
    "        name=\"Recall\",\n",
    "    ),\n",
    "],\n",
    ")\n",
    "\n",
    "callbacks = [\n",
    "    # keras.callbacks.TensorBoard(log_dir=\"logs\"),\n",
    "    keras.callbacks.ReduceLROnPlateau(patience=5),\n",
    "    # Uncomment to train your own RetinaNet\n",
    "    keras.callbacks.ModelCheckpoint(CHECKPOINT_PATH, save_weights_only=True),\n",
    "]\n",
    "\n",
    "model.fit(\n",
    "    dataset,\n",
    "    validation_data=0.3,\n",
    "    epochs=10,\n",
    "    callbacks=callbacks,\n",
    ")\n",
    "model.save_weights(CHECKPOINT_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('vision')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "eadfd090f72407bb5a6ef9b3c460c02116cfd7ec982fd272baefc156c2c66e1f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
